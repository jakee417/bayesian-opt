{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A post covering how to complete matrix inversions in [PyTorch](https://pytorch.org/) using [BLAS and LAPACK operations](https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations). This is particularly useful in Statistics for inverting [covariance matrices](https://en.wikipedia.org/wiki/Covariance_matrix) to form statistical estimators. I came across this recently during [a code review](https://github.com/pytorch/botorch/pull/2474) and wanted to collect my thoughts on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Inverse\n",
    "First, I review how to compute a matrix-inverse product exactly the way they are presented in statistical textbooks.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Ax &= b \\\\\n",
    "x &= A^{-1}b \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $A \\in \\mathbb R ^{n \\times n}$ is a [positive semi-definite (PSD)](https://en.wikipedia.org/wiki/Definite_matrix#Definitions_for_real_matrices) matrix:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x^T A x &\\geq 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For all non-zero $x \\in \\mathbb R^n$. We can solve this in PyTorch by using [`torch.linalg.inv`](https://pytorch.org/docs/stable/generated/torch.linalg.inv.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.2998, -3.1372],\n",
       "        [-0.6995, -0.0583],\n",
       "        [-1.3701, -0.7586]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "# Use double precision\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "a = torch.randn(3, 3)\n",
    "b = torch.randn(3, 2)\n",
    "# Create a PSD matrix\n",
    "A = a @ a.T + torch.eye(3) * 1e-3\n",
    "x = torch.linalg.inv(A) @ b\n",
    "x_original = x\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this is that it can become numerically instable for [poorly conditioned matrices](https://en.wikipedia.org/wiki/Condition_number#Matrices). \n",
    "\n",
    "> In general, we want to [use matrix decompositions and avoid inverting matrices](https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/) as shown in [this blog post](https://civilstat.com/2015/07/dont-invert-that-matrix-why-and-how/). \n",
    "{: .prompt-warning }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cholesky Decomposition\n",
    "\n",
    "We can avoid a matrix inverse by considering the [cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) of $A$ (giving us a lower triangular matrix $L$):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A &= LL^T \\\\\n",
    "LL^T x &= b \\\\\n",
    "L^T x &= c \\\\\n",
    "Lc &= b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Forward solve } Lc &= b \\text{ for $c$} \\\\\n",
    "\\text{Backwards solve } L^Tx &= c \\text{ for $x$} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Which can be solved efficiently using [forwards-backwards substitution](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution). In PyTorch, we can use [torch.cholesky](https://pytorch.org/docs/stable/generated/torch.cholesky.html) and [torch.cholesky_solve](https://pytorch.org/docs/stable/generated/torch.cholesky_solve.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.2998, -3.1372],\n",
       "        [-0.6995, -0.0583],\n",
       "        [-1.3701, -0.7586]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = torch.linalg.cholesky(A)\n",
    "x = torch.cholesky_solve(b, L)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6879e-15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(x, x_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, this can be accomplished through matrix multiplication and forwards-backwards algorithm without taking any matrix inverse (see [this comment](https://github.com/pytorch/pytorch/issues/77166#issuecomment-1122996050) for a description)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Decomposition\n",
    "\n",
    "We can also do this with a [LU decomposition](https://en.wikipedia.org/wiki/LU_decomposition) where $L$ is a lower triangular matrix and $U$ is an upper triangular matrix:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A &= LU \\\\\n",
    "LU x &= b \\\\\n",
    "\\text{Set } U x &= c \\\\\n",
    "Lc &= b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Forward solve } Lc &= b \\text{ for $c$} \\\\\n",
    "\\text{Backwards solve } Ux &= c \\text{ for $x$} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In PyTorch, we can use [`torch.linalg.lu_factor`](https://pytorch.org/docs/stable/generated/torch.linalg.lu_factor.html#torch.linalg.lu_factor) and [`torch.linalg.lu_solve`](https://pytorch.org/docs/stable/generated/torch.linalg.lu_solve.html#torch.linalg.lu_solve):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.2998, -3.1372],\n",
       "        [-0.6995, -0.0583],\n",
       "        [-1.3701, -0.7586]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LU, pivots = torch.linalg.lu_factor(A)\n",
    "x = torch.linalg.lu_solve(LU, pivots, b)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.5664e-16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(x, x_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDL Decomposition\n",
    "The same applies to an [LDL decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition#LDL_decomposition) which is very similar to the cholesky decomposition. LDL decomposition includes an extra diagonal matrix $D$ but avoids computing the square root of the matrix:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A &= LDL \\\\\n",
    "LDL x &= b \\\\\n",
    "\\text{Set } Lx &= c \\\\\n",
    "\\text{Set } Dc &= d \\\\\n",
    "Ld &= b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Forward solve } Ld &= b \\text{ for $d$} \\\\\n",
    "\\text{Compute } c &= D^{-1}d \\\\\n",
    "\\text{Forward solve } Lx &= c \\text{ for $x$} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> Note, we do take $D^{-1}$, but since this is a diagonal matrix, the inverse can be computed analytically by simply inverting each diagonal entry.\n",
    "{: .prompt-tip }\n",
    "\n",
    "In PyTorch, we can use (the experimental) [`torch.linalg.ldl_factor`](https://pytorch.org/docs/stable/generated/torch.linalg.ldl_factor.html) and [`torch.linalg.ldl_solve`](https://pytorch.org/docs/stable/generated/torch.linalg.ldl_solve.html#torch.linalg.ldl_solve):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.2998, -3.1372],\n",
       "        [-0.6995, -0.0583],\n",
       "        [-1.3701, -0.7586]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LD, pivots = torch.linalg.ldl_factor(A)\n",
    "x = torch.linalg.ldl_solve(LD, pivots, b)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.8664e-16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(x, x_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR Decomposition\n",
    "We can also use the [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition) where $Q$ is an orthogonal matrix and $R$ is upper right triangular matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "A &= QR \\\\\n",
    "QR x &= b \\\\\n",
    "\\text{Set } R x &= c \\\\\n",
    "Qc &= b \\\\\n",
    "c &= Q^T b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Backwards solve } Rx &= c \\text{ for $x$} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In PyTorch, we can use [`torch.linalg.qr`](https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr) and [`torch.linalg.solve_triangular`](https://pytorch.org/docs/stable/generated/torch.linalg.solve_triangular.html#torch.linalg.solve_triangular):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.2998, -3.1372],\n",
       "        [-0.6995, -0.0583],\n",
       "        [-1.3701, -0.7586]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q, R = torch.linalg.qr(A)\n",
    "c = Q.T @ b\n",
    "x = torch.linalg.solve_triangular(R, c, upper=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1020e-15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(x, x_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "We can also use the [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition#Calculating_the_SVD) where $U$ is an orthogonal matrix, $S$ is a diagonal matrix, and $V$ is a orthogonal matrix (in the special case that $A$ is a real square matrix):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A &= USV^T \\\\\n",
    "USV^Tx &= b \\\\\n",
    "x &= (VS^{-1}U^T)b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "> Note, we do take $S^{-1}$, but since this is a diagonal matrix, the inverse can be computed analytically by simply inverting each diagonal entry.\n",
    "{: .prompt-tip }\n",
    "\n",
    " In PyTorch, we can use [`torch.linalg.svd`](https://pytorch.org/docs/stable/generated/torch.linalg.svd.html#torch.linalg.svd):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.2998, -3.1372],\n",
       "        [-0.6995, -0.0583],\n",
       "        [-1.3701, -0.7586]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n",
    "x = Vh.T @ torch.diag(1 / S) @ U.T @ b\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6614e-14)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dist(x, x_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Here is a summary table of these different options:\n",
    "\n",
    "| Method   | Applicable Matrices     | Computational Complexity | Efficiency              | Stability     | Additional Benefits                    |\n",
    "|----------|-------------------------|--------------------------|-------------------------|---------------|----------------------------------------|\n",
    "| Cholesky | Symmetric positive def. | $O(n³/3)$                  | Highest for applicable  | Very good     | Memory efficient                       |\n",
    "| LU       | Square                  | $O(2n³/3)$                 | Good for general cases  | Good w/ pivot | Useful for determinants and inverses   |\n",
    "| LDL      | Symmetric               | $O(n³/3)$                  | Good for symmetric      | Good          | Avoids square roots                    |\n",
    "| QR       | Any                     | $O(2mn² - 2n³/3)$ for $m≥n$  | Less than LU for square | Very good     | Best for least squares                 |\n",
    "| SVD      | Any                     | $O(\\min(mn², m²n))$         | Lowest                  | Excellent     | Best for ill-conditioned systems       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blackbox Matrix-Matrix Multiplication (BBMM)\n",
    "Since matrix inversion is especially relevant to [Gaussian Processes](https://en.wikipedia.org/wiki/Gaussian_process), the library [GPyTorch](https://gpytorch.ai/) has implemented a [Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration](https://arxiv.org/abs/1809.11165) library. Importantly, it lowers the cost of the above approaches from $O(n^3)$ to $O(n^2)$ and allows routines to be used on GPU architectures. GPyTorch uses [LinearOperator](https://github.com/cornellius-gp/linear_operator) which is useful for exploiting specific matrix structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_operator.operators import DiagLinearOperator, LowRankRootLinearOperator\n",
    "C = torch.randn(1000, 20)\n",
    "d = torch.ones(1000) * 1e-9\n",
    "b = torch.randn(1000)\n",
    "A = LowRankRootLinearOperator(C) + DiagLinearOperator(d)\n",
    "_ = torch.linalg.solve(A, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
